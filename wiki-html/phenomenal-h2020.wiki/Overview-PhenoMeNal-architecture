<p>The PhenoMeNal VRE consists of the following main components: <br />
* Software tools which are standardised and wrapped as software containers
* Standardised and interoperable data formats
* VRE contextualisation scripts to launch it on an <strong>Infrastructure-as-a-Service</strong> (IaaS) resources from public providers such as Google Cloud Platform, Amazon Web services; private OpenStack installations, or standalone computers.</p>

<p>PhenoMeNal implements a microservices architecture, where data analysis consists of connecting tools together to form an analysis pipeline. Data formats are all agreed-upon and following open standards like <strong>mzML</strong> (Mass Spectrometry Markup Language), <strong>nmrML</strong> (NMR markup language) and <strong>ISA-Tab</strong>, enabling simplification of communication and data sent between tools. These tools are available as containers that can be easily deployed without manual installation and dependency management. These containers, in an elastic IT-environment, scale out to run analysis in parallel on multiple compute nodes. All technical details are transparent for the metabolomics researcher.</p>

<p>The stack-based diagram below explains the PhenoMeNal architecture and chosen implementations:</p>

<p><img src="images/architecture/PhenoMeNal_architecture.png" alt="PhenoMeNal Architecture" /></p>

<p>Figure: The PhenoMeNal architecture (right) with selected implementations, depicted as a stack diagram and aligned to general microservice-based architectures (left).</p>

<p>On the lowest level is the actual <strong>hardware</strong>; computer or a virtual <em>cloud</em> running on a cluster miles away. The user makes use of provisioning software to prepare and equip the virtual cluster with necessary software-layers. This often starts with a system kernel, which controls the very basics of the computer system. The kernel is the intermediary between the hardware (possibly virtual) and OS. It deals with resource management, load-balancing, runtime scheduling and more. Every single node runs its own kernel and OS, with a cluster OS layered on top as an abstraction-layer, making it appear as if all the nodes are part of one big computer. Combining the fundamental functions provided by the kernel with a Cluster OS of choice results in a virtual cluster with combined resources and the ability to split workloads between nodes as if they were all part of the same physical machine. The operating system then takes over and handles most of the communication.</p>

<p>With the operating system in place the desired services can be installed. In order to be able to mount and run containers containing microservices, a container engine is needed. The main function of it is supporting the launching, scaling, management and termination of its auxiliary containers. It is through the <strong>container engines</strong> API that all container orchestration software operates.</p>

<p><strong>Containers</strong> are pieces or parts of a program running within a closed virtual environment containing only the files needed for it to function. This makes a container entirely independent of the surrounding software environment, which is advantageous because it can be moved to and run on any operating system having the required container engine. In this use-case where microservices are wrapped up in software containers this means they are easy to add, remove and rearrange for the desired workflow.</p>

<p>The microservices run within these containers are all independent functions, usually from existing software packages. Containerizing these functions comes with several benefits, where their quick launch is one of the most important. This results in fast and simple scalability as required, since additional virtual nodes can be added to the virtual cluster, provisioned with all the software needed and then supplied with the necessary container. In a fraction of the time it would take to build, configure and install additional physical machines, a virtual cluster can accommodate for heavier workloads.</p>

<h2>The PhenoMeNal stack</h2>

<p>PhenoMeNal is built to run on private machine as well as with any Infrastructure-as-a-Service-provider. It uses the <strong>MANTL</strong> suite of tools for most of its functions with Terraform as the infrastructure builder of choice. It gives the user simple script-based control over the launch and management of their infrastructure. <strong>Ansible</strong> is used as provisioning software, installing both kernel, operating system and engines. <strong>Mesos</strong> and/or <strong>Kubernetes</strong> gathers the cluster of nodes to a single workspace and functions as its kernel and OS. <strong>Docker</strong> is the container-environment of choice and Ansible supplies its engine along with the dependencies. Kubernetes and Mesos functions overlap but within PhenoMeNal the main function of Kubernetes is container orchestration. The desired analysis functions are downloaded as small independent Docker containers and mounted through Kubernetesâ€™ orchestration tools. The Figure below shows an overview of the interacting components inside a running VRE.</p>

<p><img src="images/architecture/PhenoMeNal_architecture_1.png" alt="PhenoMeNal Architecture" /></p>

<p>Figure: Overview of the interacting components inside a running PhenoMeNal VRE deployed using Mesos. The control nodes are redundant services enabling the functions of the systems in a fault-tolerant way. The Edge nodes manage the network connection with the user. PhenoMeNal currently uses Jupyter and Galaxy as graphical web front-ends with traffic routed through these Edge nodes. Workflow engines such as Galaxy, manage dependency graphs and communicates with Kubernetes that handles the orchestration of containers using the docker engine.</p>
